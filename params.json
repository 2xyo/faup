{"name":"Faup","tagline":"Fast URL decoder library","body":"<p align=\"center\"><img src=\"doc/images/faup-logo.png\"/></p>\r\n\r\n**Faup** stands for Finally An Url Parser and is a library and command line tool to parse URLs and normalize fields with two constraints:\r\n 1. Work with real-life urls (resilient to badly formated ones)\r\n 2. Be fast: no allocation for string parsing and read characters only once\r\n\r\n\r\n * **Source**: [https://github.com/stricaud/faup][github]\r\n * **Issues**: [https://github.com/stricaud/faup/issues][issues]\r\n * **Mailing List**: [libfaup@googlegroups.com](https://groups.google.com/d/forum/libfaup)\r\n\r\n## Documentation\r\n\r\n* [Library API documentation][libdoc]\r\n* [Command Line Tool][clidoc]\r\n\r\n## Quick Start\r\n\r\nWhat is provided?\r\n-----------------\r\n\r\n* A static library you can embed in your software (faup_static)\r\n* A dynamic library you can get along with (faupl)\r\n* A command line tool you can use to extract various parts of a url (faup)\r\n\r\nWhy Yet Another URL Extraction Library?\r\n---------------------------------------\r\n\r\nBecause they all suck. Find a library that can extract, say, a TLD even if you have \r\nan IP address, or http://localhost, or anything that may confuse your regex so much\r\nthat you end up with an unmaintainable one.\r\n\r\nArchitecture\r\n------------\r\n\r\n[ URL ] -> [ Features discovery ] -> [ Decoding ] -> [ URL Fields ]\r\n\r\nCommand line usage\r\n------------------\r\n\r\nSimply pipe or give your url as a parameter:\r\n\r\n\t$ echo \"www.github.com\" |faup -p\r\n\tscheme,credential,subdomain,domain,host,tld,port,resource_path,query_string,fragment\r\n\t,,www,github.com,www.github.com,com,,,,\r\n\r\n\t$ faup www.github.com\r\n\t,,www,github.com,www.github.com,com,,,,\r\n\r\nExtract TLD > 1\r\n---------------\r\n\r\nWe use the mozilla list that you can update like this:\r\n\r\n      $ faup -u\r\n\r\nThen the -t flag will search for TLDs against the mozilla list (and we output as json for clarity):\r\n\r\n     $ faup -o json -t http://www.google.co.uk\r\n     {\r\n\t\"scheme\": \"http\",\r\n\t\"credential\": \"\",\r\n\t\"subdomain\": \"www\",\r\n\t\"domain\": \"google.co.uk\",\r\n\t\"host\": \"www.google.co.uk\",\r\n\t\"tld\": \"co.uk\",\r\n\t\"port\": \"\",\r\n\t\"resource_path\": \"\",\r\n\t\"query_string\": \"\",\r\n\t\"fragment\": \"\",\r\n     }\r\n\r\nExtract only the TLD field\r\n--------------------------\r\n\r\n\t$ faup -f tld slashdot.org\r\n\torg\r\n\r\n\t$ faup -f tld www.bbc.co.uk\r\n\tuk\r\n\r\n\t$ faup -f tld -t www.bbc.co.uk\r\n\tco.uk\r\n\r\nOf course, without the -t flag we are faster because we are not checking against a list.\r\n\r\n\r\nPython bindings\r\n---------------\r\n\r\nHere's what you can do:\r\n\r\n       >>> from pyfaup.faup import Faup\r\n       >>> f = Faup()\r\n       >>> f.decode(\"https://www.slashdot.org\")\r\n       >>> f.get()\r\n       {'credential': None, 'domain': 'slashdot.org', 'subdomain': 'www', 'fragment': None, 'host': 'www.slashdot.org', 'resource_path': None, 'tld': 'org', 'query_string': None, 'scheme': 'https', 'port': None}\r\n       >>> \r\n\r\nC API\r\n-----\r\n\r\nAgain, things are basic:\r\n\r\n       faup_handler_t *fh;\r\n\r\n       fh = faup_init();\r\n       faup_decode(fh, \"https://wallinfire.net\", strlen(\"https://wallinfire.net\"));\r\n       tld_pos = faup_get_tld_pos(fh); /* will return 19 */       \r\n       tld_size = faup_get_tld_size(fh); /* will return 3 */       \r\n       faup_show(fh, ',', stdout);\r\n\r\n       faup_terminate(fh);\r\n\r\nBuilding faup\r\n-------------\r\n\r\nTo get and build faup, you need [cmake](http://www.cmake.org/). As cmake doesn't allow\r\nto build the binary in the source directory, you have to create a build directory.\r\n\r\n    git clone git://github.com/stricaud/faup.git\r\n    cd faup\r\n    mkdir build\r\n    cd build\r\n    cmake .. && make\r\n\r\n[github]: https://github.com/stricaud/faup\r\n[issues]: https://github.com/stricaud/faup/issues\r\n[libdoc]: doc/library.md\r\n[clidoc]: doc/cli.md\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}